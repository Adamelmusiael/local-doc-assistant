# Server-Sent Events (SSE) Streaming Implementation

## Overview

This implementation adds real-time streaming chat responses to the AI Assistant, similar to ChatGPT's streaming interface. Users see responses appear word-by-word as they are generated by the LLM.

## What Was Implemented

### 1. New Streaming Endpoint
- **URL**: `POST /chat/{session_id}/stream`
- **Response Type**: Server-Sent Events (SSE)
- **Content-Type**: `text/plain` with SSE format

### 2. Streaming Message Handler
- **Function**: `handle_chat_message_stream()` in `src/chat_logic/message_handler.py`
- **Features**: 
  - Yields chunks as they are generated
  - Provides status updates (searching, generating)
  - Sends source information
  - Includes metadata and error handling

### 3. LLM Streaming Support
- **OpenAI**: Uses `stream=True` parameter
- **Local Models (Ollama)**: Handles streaming responses via `stream=True` in API calls
- **Error Handling**: Graceful fallback for connection issues

## SSE Message Format

The streaming endpoint sends different types of messages:

### Message Types

```json
// Start of stream
{"type": "start", "session_id": 1, "model": "mistral"}

// Status updates
{"type": "status", "content": "Searching documents..."}
{"type": "status", "content": "Generating response..."}

// Document sources
{
  "type": "sources", 
  "content": [
    {"text": "chunk text", "score": 0.95, "metadata": {...}}
  ],
  "chunks_used": 3
}

// Response chunks (the actual AI response)
{"type": "chunk", "content": "Hello"}
{"type": "chunk", "content": " there"}
{"type": "chunk", "content": "!"}

// Final metadata
{
  "type": "metadata",
  "content": {
    "model": "mistral",
    "confidence": null,
    "hallucination": null,
    "search_mode": "all",
    "total_response": "Hello there!"
  }
}

// Stream completion
{"type": "done"}

// Error handling
{"type": "error", "content": "Error message"}
```

## How It Works

### Backend Flow
1. **Client sends POST** to `/chat/{session_id}/stream`
2. **Document search** - Find relevant chunks
3. **Prompt building** - Combine context with user question
4. **Stream generation** - LLM generates response in chunks
5. **Real-time sending** - Each chunk sent immediately via SSE
6. **Database storage** - Complete response saved when done

### Frontend Integration (Next Step)
```javascript
// JavaScript example for frontend
const eventSource = new EventSource('/api/chat/1/stream');

eventSource.onmessage = function(event) {
  const data = JSON.parse(event.data);
  
  switch(data.type) {
    case 'chunk':
      // Append text to chat message
      appendToMessage(data.content);
      break;
    case 'sources':
      // Show document sources
      displaySources(data.content);
      break;
    case 'done':
      // Complete the message
      eventSource.close();
      break;
  }
};
```

## CORS Configuration

Updated CORS to support SSE:
- Added streaming headers
- Disabled buffering for real-time response
- Proper cache control headers

## Testing

### 1. Using curl
```bash
curl -X POST "http://localhost:8000/chat/1/stream" \
     -H "Content-Type: application/json" \
     -d '{"question": "Hello", "model": "mistral"}' \
     --no-buffer
```

### 2. Browser JavaScript
```javascript
fetch('/api/chat/1/stream', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({question: "Hello", model: "mistral"})
})
.then(response => {
  const reader = response.body.getReader();
  // Read stream chunks
});
```

### 3. Health Check
First verify the basic endpoint works:
```bash
curl http://localhost:8000/health
```

## Benefits

### User Experience
- ✅ **Real-time responses** - No waiting for complete generation
- ✅ **Interactive feeling** - Like chatting with a human
- ✅ **Progress feedback** - See search and generation status
- ✅ **Source attribution** - Know which documents were used

### Technical
- ✅ **Efficient** - Send data as it's generated, not all at once
- ✅ **Scalable** - Doesn't block server while generating
- ✅ **Reliable** - Proper error handling and reconnection
- ✅ **Compatible** - Works with both OpenAI and local models

## Next Steps

1. **Frontend Integration** - Connect React app to streaming endpoint
2. **UI Components** - Create streaming message components
3. **Error Handling** - Add retry logic and connection management
4. **Optimizations** - Add typing indicators and better status updates

## Error Handling

The implementation includes comprehensive error handling:
- **Model errors** - Invalid models, API failures
- **Connection errors** - Network issues, timeouts
- **Streaming errors** - Malformed responses, interruptions
- **Database errors** - Storage failures

All errors are sent as SSE messages with type "error" so the frontend can handle them gracefully.
